package com.nijika;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
import org.apache.hadoop.hbase.mapreduce.TableInputFormat;
import org.apache.hadoop.hbase.util.Bytes;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.sql.SparkSession;
import scala.Tuple2;

public class SparkDebug {
    private static final String ZK = "ubuntu-linux-2404";

    public static void main(String[] args) {
        System.setProperty("HADOOP_USER_NAME", "hadoop");
        SparkSession spark = SparkSession.builder()
                .master("local[*]")
                .appName("Debug_HBase")
                .getOrCreate();

        System.out.println("========== ğŸ” å¼€å§‹ HBase è¿é€šæ€§è¯Šæ–­ ==========");

        try {
            Configuration conf = HBaseConfiguration.create();
            conf.set("hbase.zookeeper.quorum", ZK);
            conf.set(TableInputFormat.INPUT_TABLE, "tourism:global_rank");

            JavaRDD<Tuple2<ImmutableBytesWritable, Result>> rdd = spark.sparkContext()
                    .newAPIHadoopRDD(conf, TableInputFormat.class, ImmutableBytesWritable.class, Result.class)
                    .toJavaRDD();

            JavaRDD<String> resultRDD = rdd.map(tuple -> {
                Result result = tuple._2;
                String rowKey = Bytes.toString(result.getRow());
                byte[] val = result.getValue(Bytes.toBytes("info"), Bytes.toBytes("city"));
                String city = (val != null) ? Bytes.toString(val) : "NULL";
                return "RowKey: " + rowKey + ", City: " + city;
            });

            long count = rdd.count();
            System.out.println(">>> [è¯Šæ–­ç»“æœ] HBase 'tourism:global_rank' è¡¨ä¸­è¯»å–åˆ°: " + count + " æ¡æ•°æ®");

            if (count > 0) {
                System.out.println(">>> [æ•°æ®æŠ½æ ·] ç¬¬ä¸€æ¡æ•°æ®å†…å®¹:");
                // âœ… Only collect the String result (which IS serializable)
                String firstResult = resultRDD.first();
                System.out.println(firstResult);
            } else {
                System.out.println(">>> âš ï¸ è­¦å‘Š: Spark è¿ä¸Šäº† HBaseï¼Œä½†æ²¡è¯»åˆ°æ•°æ®ï¼");
                System.out.println(">>> å¯èƒ½åŸå› : 1. è¡¨çœŸæ˜¯ç©ºçš„ 2. Mac æ— æ³•è¿æ¥ RegionServer (æ£€æŸ¥ /etc/hosts)");
            }

        } catch (Exception e) {
            e.printStackTrace();
        }

        spark.stop();
    }
}